{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94dbb26",
   "metadata": {
    "papermill": {
     "duration": 0.002364,
     "end_time": "2023-07-04T08:11:19.834753",
     "exception": false,
     "start_time": "2023-07-04T08:11:19.832389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align: center;\">Ethics, AIs, and the Law </h1> \n",
    "\n",
    "<h2 style=\"text-align: left;\">Abstract </h2> \n",
    "<h3 style=\"text-align: left;\">As AI technology becomes more ubiquitous, so do calls for an ethical frameworks.</h3> <br>\n",
    "<br>\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=wTV-Y_evPsg\n",
    "\" target=\"_blank\"><img align=\"left\"  src=\"http://img.youtube.com/vi/wTV-Y_evPsg/hqdefault.jpg\" \n",
    "alt=\"AI : Fairness, Trust, and the Law.\" width=\"240\" height=\"180\" border=\"10\" /></a>\n",
    "<h4 style=\"text-align: left;\">\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;I broadly address the question of [‘Why Ethics and Why now?'](https://youtu.be/wTV-Y_evPsg),<br>\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;in a video (10 mins.long, a transcript of the video is available in the [Transcript section](#vidtext),<br>\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;In the second part, which follows, I narrow the focus on techniques that, ,<br>\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;I suggest, are best suited to fulfil some of the requirements listed in <br>\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;guidelines for an Ethical AI, which will be at the core of coming regulation.</h4>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<h3 style=\"text-align: left;\">'Ethics: a changing landscape' </h3> \n",
    "<br>\n",
    "An insight in of how the field has changed, in less than three years, can be gained from Artificial Intelligence Index Report (HAI)<br>\n",
    "\n",
    "Review year 2021 – highlights [(00)](#ref),\n",
    "\n",
    "> “The five news topics that got the most attention in 2020 related to the ethical use of AI were the release of the European Commission’s white paper on AI, Google’s dismissal of ethics researcher Timnit Gebru, the AI ethics committee formed by the United Nations, the Vatican’s AI ethics plan, and IBM’s exiting the facial-recognition businesses.”,\n",
    "\n",
    "here the attention is on a handful of organization. <br>\n",
    "\n",
    "Review year 2022 – highlights [(01)](#ref),\n",
    ">* Language models are more capable than ever, but also more biased[...]\n",
    ">* The rise of AI ethics everywhere: Research on fairness and transparency in AI has exploded since 2014, with a fivefold increase in related publications at ethics-related conferences[...] \n",
    ">* multimodal language-vision models [...] reflect societal stereotypes and biases in their outputs,\n",
    "<br>\n",
    "\n",
    "Concerns begin to emerge on the negative impacts of the technology on human society.<br>\n",
    "\n",
    "Review year 2023 – highlights [(02)](#ref),\n",
    ">* ...Text-to-image generators are routinely biased along gender dimensions, and chatbots like ChatGPT can be tricked into serving nefarious aims. \n",
    ">* The number of incidents concerning the misuse of AI is rapidly rising. \n",
    ">* ...Language models which perform better on certain fairness benchmarks tend to have worse gender bias. \n",
    ">* ...The number of accepted submissions to FAccT, a leading AI ethics conference, has more than doubled since 2021 \n",
    ">* ...several benchmarks have been developed for automated fact-checking, researchers find that 11 of 16 of such datasets rely on evidence “leaked” from fact-checking reports which did not exist at the time of the claim surfacing. ,\n",
    "\n",
    "The focus is firmly on how abuse, misuse, and inadequate design are having a negative impact on society.  <br>\n",
    "As the use of AI systems becomes more widespread, so do papers trying to address how to best implement Ethics in the AI field. A quick search on arxiv.org for AI and Ethics covering the past two and half years, returns just above 500 results. In the graph below, the results have been grouped in six months intervals. Notably, there is a marked increase in the pace of publication in 2023.<br>\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https://github.com/Danddt/AI_you_tube_vids/blob/main/ai_ethics_papers_graph.jpg?raw=true\"> </div> \n",
    "\n",
    "<br>\n",
    "Most of the literature of the past two and a half years share concerns over the difficulties of closing the gap between the theory and the practice. Although a Grand Model for AI Ethics is desirable, a more piecemeal, but timely and deliverable, approach may have to take precedence. \n",
    "<br>\n",
    "In 2021, there were already calls for a more practical approach:<br>\n",
    "\n",
    ">“Rather than attempting to codify ethics, ethics-based auditing helps identify, visualise, and communicate whichever normative values are embedded in a system.”, [(1)](#ref),\n",
    "\n",
    "and how interpretability can contribute to model Transparency,\n",
    " >“… interpretability [...] to bring transparency to the black-box AI systems, which is such an important feature in the context of AI ethics.” [(2)](#ref)\n",
    " \n",
    "Bridging the gap between Theory and Practice and focusing on finding implementable solutions continued to be key in 2022,  \n",
    ">“… Though existing ethical guidelines match ethical issues in the real world, the ethical rules are too theoretical and vague,[...] people know the rules but do not know how to implement...”, [(3)](#ref).\n",
    "\n",
    "In 2023, Miller suggests expanding Explainability techniques, so that, depending on context, the system output can be used to produce recommendations, as well as,\n",
    ">\"(it) supports the decision maker’s cognitive process by allowing them to explore hypotheses, rather than providing only the information that justifies a machine recommendation…\",[(4)](#ref).\n",
    "\n",
    "The following graph highlights requirements set forth by guidelines issued by a number of bodies for a Trustworthy/Ethical AI: <br>\n",
    "\n",
    "![](https://github.com/Danddt/AI_you_tube_vids/blob/main/guidlines_graphby2.jpg?raw=true)\n",
    "\n",
    "The most commonly sighted requirements are Explainability, Transparency and Accountability. I suggest, that explaining the model and how it reaches its conclusion can address, to a reasonable extent, Explainability, Transparency and Accountability, and by submitting the explanation for review by domain experts and historical statistical analysis (Bayesian statistics) provide insights in the likely accuracy of the predictions in real-life, and highlight the presence of bias. Explainable AI (XAI), is a vast field, here however I highlight work being done on explanatory established techniques such as, SHAP, and LIME, and some promising emerging techniques such as Sequential Integrated Gradients and MaNtLe.\n",
    "\n",
    "Recent work by Joran Michiels et. al.[(5)](#ref), the authors propose a method for addressing both model and results explainability:<br> \n",
    ">“SHAP value for a feature can be split up into a direct (model corresponding) attribution and an attribution via other dependent features, allowing the end-user (...) to easily grasp the explanation of both the model and the result.”\n",
    "\n",
    "While research to expand and improve SHAP is thriving, by contrast there is little work focusing on LIME. C. Burger et al. [(6)](#ref) suggest that explanations do not always remain stable when unimportant words are perturbed (circa: ‘adding noise), additionally difficulties with selecting an optimal kernel persist, and the model is computationally expensive [(7)](#ref).\n",
    "\n",
    "J. Enguehard [(8)](#ref), proposes ‘ Sequential Integrated Gradients’ to create an interpolation only between the baseline and the word of interest, to protect the intended meaning,<br>\n",
    ">“(…) keeping the meaning of interpolated sentences close to the original one is key to producing good explanations”.\n",
    " \n",
    "R.R. Menon et al. [(9)](#ref) propose a model-agnostic natural language explainer, MaNtLe,  that generates explanations for structured classifiers decision-making that are more human-reader friendly.\n",
    "\n",
    "Next, I touch on the role that synthetic data can play in ensuring the AI models meet privacy and data protection requirements, as well as help with managing bias. In  “Synthetic Data - what, why and how?”[10], the authors suggest that, although synthetic data is not immune to the same issues that affect real data, it can be cost-efficient, speedy and comply with privacy laws, in addition it has the capability to produce large amounts of data, even when the original data-set is too small to ensure AI models accuracy. Traditional approaches, for synthetic data generation, include SMOTE [(11)](#ref), and ADASYN[(12)](#ref), however interesting work on using GAN to generate tabular data has been put forwards by N. Venkata Chereddy and B. Kumar Bolla[(13)](#ref), the authors suggest that GANs can effectively predict rare case scenarios. An alternative approach is presented by E. Kreaˇci´c et al.[(14)](#ref) the authors propose a ‘transparent algorithm’ for data generation. This last approach is attractive as, proving that it complies with requirements of Explainability, Transparency and Fairness might be easier than for GAN generated data. \n",
    "\n",
    "To conclude, the field of AI ethics is growing, and although the philosophical debate is alive and well, the focus is on providing practical solutions. This shift may be due to, firstly, an increase in the number of AI incidents, secondly the incoming regulatory framework is putting pressure on the field to produce systems that meet ethical requirements.\n",
    "Geo-political tensions and economic turmoil affect the ability or willingness of companies to invest in new technology. Additionally, companies may choose to delay investment in AI systems, until it becomes clear how regulation will affect the field. Finally economic, political, and resource constraints, brought about by national and international events,  will put pressure on developers to demonstrate the benefits of investing in an AI system, versus a, perhaps, cheaper, traditional code-based system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f75224",
   "metadata": {
    "papermill": {
     "duration": 0.001415,
     "end_time": "2023-07-04T08:11:19.838063",
     "exception": false,
     "start_time": "2023-07-04T08:11:19.836648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"text-align: left;\">References: </h3> \n",
    "<div id=\"ref\"> \n",
    "HAI (by Stanford University)<br>\n",
    "(00): https://aiindex.stanford.edu/ai-index-report-2021/<br>\n",
    "(01): https://aiindex.stanford.edu/ai-index-report-2022/<br>\n",
    "(02): https://aiindex.stanford.edu/report/<br>\n",
    "https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report-2023_CHAPTER_3.pdf (specifically)<br>\n",
    "\n",
    "(1)'Ethics‑Based Auditing to Develop Trustworthy AI' J. Mökander L.Floridi 9-Feb-2021, https://arxiv.org/ftp/arxiv/papers/2105/2105.00002.pdf<br>\n",
    "(2)'Interpretable deep-learning models to help achieve the Sustainable Development Goals' R. Vinuesaa, B. Sirmacekc, 24-Aug -2021, https://arxiv.org/pdf/2108.10744.pdf<br>\n",
    "(3)'AI ETHICS ISSUES IN REAL WORLD: EVIDENCE FROM AI INCIDENT DATABASE' M. Wei, Z. Zhou, 18-Aug-2022, https://arxiv.org/pdf/2206.07635.pdf<br>\n",
    "(4)'EXPLAINABLE AI IS DEAD, LONG LIVE EXPLAINABLE AI! HYPOTHESIS-DRIVEN DECISION SUPPORT' T.Miller, 11-Mar-2023, https://arxiv.org/pdf/2302.12389.pdf<br>\n",
    "(5)‘Explaining the Model and Feature Dependencies by Decomposition of the Shapley Value⋆’, Joran Michielsa, Maarten De Vosa, Johan Suykensa, 19-Jun-2023 (https://arxiv.org/pdf/2306.10880.pdf)<br>\n",
    "(6)‘Are Your Explanations Reliable?\" Investigating the Stability of LIME in Explaining Textual Classification Models via Adversarial Perturbation’ , Christopher Burger, Lingwei Chen, Thai Le, 21-May-2023, https://arxiv.org/pdf/2305.12351.pdf<br>\n",
    "(7)A Guide for Making Black Box Models Explainable (2nd ed.),Christoph Molnar,\n",
    "christophm.github.io/interpretable-ml-book/ - chapter : 9.2 Local Surrogate (LIME)<br>\n",
    "(8)'Sequential Integrated Gradients: a simple but effective method for explaining language models',25-May-2023, Joseph Enguehard, https://arxiv.org/pdf/2305.15853.pdf)<br>\n",
    "(9)'MaNtLE: Model-agnostic Natural Language Explainer', Rakesh R Menon, Kerem Zaman, Shashank Srivastava, 22-May-2023, https://arxiv.org/pdf/2305.12995.pdf\n",
    "(10)'Synthetic Data - what, why and how?', Alan Turing Institute, 6-May-2022,https://arxiv.org/pdf/2205.03257.pdf<br>\n",
    "(11)SMOTE: (Synthetic Minority Over sampling Technique )\n",
    "(12)ADASYN: (ADAptive SYNthetic: adaptively generating minority data samples according to their distributions using K nearest neighbor. https://www.datasciencecentral.com/handling-imbalanced-data-sets-in-supervised-learning-using-family/<br>\n",
    "(13)‘Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings’, Nagarjuna Venkata Chereddy, Bharath Kumar Bolla, Jun-2023, https://arxiv.org/ftp/arxiv/papers/2306/2306.13929.pdf<br>\n",
    "(14)“Differentially Private Synthetic Data Using KD-Trees”,Eleonora Kreaˇci´c, Navid Nouri, Vamsi K. Potluru, Tucker Balch, Manuela Veloso, 19-Jun-2023, https://arxiv.org/pdf/2306.13211.pdf<br>\n",
    "\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/alexisbcook/ai-fairness<br>\n",
    "https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights<br>\n",
    "\n",
    "‘Predicting Neural Network Accuracy from Weights’,Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, Ilya Tolstikhin, 9-Apr-2023, https://arxiv.org/pdf/2002.11448.pdf<br>\n",
    "\n",
    "Opening up the Black Box:<br>\n",
    "NVIDIA:\n",
    "https://www.nvidia.com/en-us/glossary/data-science/xgboost/<br>\n",
    "SHAP:\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html<br>\n",
    "LIME:\n",
    "https://c3.ai/glossary/data-science/lime-local-interpretable-model-agnostic-explanations/<br>\n",
    "\n",
    "On Synthetic data:<br>\n",
    "https://www.aitude.com/problems-with-synthetic-data/\n",
    "https://datagen.tech/guides/synthetic-data/synthetic-data-generation/#<br>\n",
    "\n",
    "Issues with AI models used in the judicial system:\n",
    "https://onlinelibrary.wiley.com/doi/full/10.1002/bsl.2456<br>\n",
    "Self Driving cars:\n",
    "Summary Report: Standing General Order on Crash Reporting for Automated Driving Systems\n",
    "https://www.nhtsa.gov/sites/nhtsa.gov/files/2022-06/ADS-SGO-Report-June-2022.pdf<br>\n",
    "\n",
    "Keep up to date with the debate on AI Ethics/Fiarness:<br>\n",
    "-- https://facctconference.org/2023/acceptedpapers.html ---\n",
    "https://facctconference.org/index.html<br>\n",
    "\n",
    "European Union Guidelines/Regulation:\n",
    "https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence?&at_campaign=20226-Digital&at_medium=Google_Ads&at_platform=Search&at_creation=RSA&at_goal=TR_G&at_advertiser=Webcomm&at_audience=%7bkeyword%7d&at_topic=Artificial_intelligence_Act&at_location=BG&gclid=CjwKCAjwv8qkBhAnEiwAkY-ahsVqaKePe59Qz2WauWM9PRHBN7tavTWJ7GKl8miYgUQ6ilvpQjiYvxoCKMAQAvD_BwE\n",
    "pdf on: https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf<br>\n",
    "Guidlines for AI by EU 2019:\n",
    "https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai \n",
    "https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment<br>\n",
    "\n",
    "Art. 17 GDPR Right to erasure (‘right to be forgotten’) https://gdpr-info.eu/art-17-gdpr/<br>\n",
    "\n",
    "United States Guidelines/Regulation:\n",
    "https://www.ai.gov/strategy-documents/<br>\n",
    "US-DoD reg:\n",
    "https://www.defense.gov/News/Releases/Release/Article/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/<br>\n",
    "\n",
    "OECD:\n",
    "https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449<br>\n",
    "\n",
    "UNESCO:\n",
    "https://unesdoc.unesco.org/ark:/48223/pf0000380455<br>\n",
    "\n",
    "UN:\n",
    "https://unsceb.org/sites/default/files/2022-09/Principles%20for%20the%20Ethical%20Use%20of%20AI%20in%20the%20UN%20System_1.pdf<br>\n",
    "\n",
    "NATO:\n",
    "https://www.nato.int/cps/en/natohq/official_texts_187617.htm<br>\n",
    "\n",
    "News Articles:<br>\n",
    "https://www.washingtonpost.com/technology/2023/06/10/tesla-autopilot-crashes-elon-musk/<br>\n",
    "https://www.theverge.com/2023/6/21/23768257/ai-schumer-safe-innovation-framework-senate-openai-altman<br>\n",
    "https://www.theverge.com/2023/5/4/23710533/google-microsoft-openai-white-house-ethical-ai-artificial-intelligence<br>\n",
    "https://www.axios.com/2023/04/13/congress-regulate-ai-tech?utm_source=newsletter&utm_medium=email&utm_campaign=newsletter_axiosam&stream=top<br>\n",
    "https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt<br>\n",
    "https://www.politico.eu/article/chatgpt-world-regulatory-pain-eu-privacy-data-protection-gdpr/<br>\n",
    "\n",
    "Other:\n",
    "Ethics Philosophers list:\n",
    "https://www.britannica.com/topic/ethics-philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa8c72",
   "metadata": {
    "papermill": {
     "duration": 0.001358,
     "end_time": "2023-07-04T08:11:19.841072",
     "exception": false,
     "start_time": "2023-07-04T08:11:19.839714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"text-align: left;\">Transcript -Video: </h2> \n",
    "<div id=\"vidtext\"> \n",
    "<h3 style=\"text-align: left;\">AI : Fairness, Trust, and the Law.</h3><br>\n",
    "Judging is central to being human, we judge everything, events, places, food, books, stop or start at a road junction, and people. And we expect to be judged by people, but increasingly judgment is made by machines, a book review may have been produced by an AI, a loan application may be approved or not by an AI, your health score, the results of your exams, your suitability for a job, even our capacity for crime. These systems produce results fast, for example a loan application can be approved in minutes, an AI can go through hundreds of scans and flag up those that need further investigation by a physician faster than a purely human team, these are just a few examples. But as these systems become more widespread, and impact more aspects of people’s lives, the trustworthiness of these systems becomes critical. <br>\n",
    "AIAAIC,(https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report-2023_CHAPTER_3.pdf, pg.9), has been recording AI incidents, and the number of incidents in 2021 was 26 times greater than in 2012. This rise suggests that as more AI systems are released into the wild, people become more aware of incidents of misuses, bias, and failures to perform as advertised: \n",
    "Deep fakes are examples of misuse, one such example of misuse is President Zelenskyy’s call for surrender to the Russian Army video. Other examples of misuse is when AI technology, such as speech recognition and face recognition are used in ways that may breach people’s privacy, such as when used on inmates conversations, or used to monitor pupils’ emotions. Another example is the misuse of ChatGPT, by  lawyers 2 lawyers(ref: https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt).<br>\n",
    "Another instance where AI systems have come under scrutiny is in the criminal justice system, where the accuracy of their predictions is being questioned,in ‘Judicial gatekeeping on scientific validity with risk assessment tools’, M. Hamilton ( https://onlinelibrary.wiley.com/doi/full/10.1002/bsl.2456) suggests that the model output becomes increasingly inaccurate for types of crime and perpetrators that are not very common.   <br>\n",
    "And instances where AI systems appear to have failed to perform as advertised, as in the case of accidents involving cars with self-driving technologies, these are now being actively recorded and investigated by (https://www.nhtsa.gov/sites/nhtsa.gov/files/2022-06/ADS-SGO-Report-June-2022.pdf)<br>\n",
    "When a person makes a mis-judgement, societies have a large body of rules developed over thousands of years to deal with it, most are centred on intent, a genuine error versus malicious intent, and the gravity of the consequences of said act to property and other people. Some of these rules cannot always be directly transferred onto AI systems. <br>\n",
    "A number of organizations have put forward ethical guidelines for a trustworthy AI, these include but are not limited to UNESCO, the UN, the OECD, at a national level the US, the EU, the United Kingdom, Japan, Canada, Iceland, Norway, the United Arab Emirates, India, Singapore, South Korea Australia, and China. <br>\n",
    "Looking at the EU case in specific, a High-Level Expert Group on AI was set up in 2018, and in 2019 it produced a set of guidelines: <br>\n",
    "A trustworthy AI should be: <br>\n",
    "    \n",
    "* (1) lawful -  respecting all applicable laws and regulations\n",
    "  \n",
    "* (2) ethical - respecting ethical principles and values\n",
    "  \n",
    "* (3) robust - both from a technical perspective while taking into account its social environment<br>\n",
    "\n",
    "these principles are echoed in guidelines put forwards by other bodies. This graph helps getting an idea of the most common requirements,  it is very reductive, but it works as a starting point, <br>\n",
    "![](https://github.com/Danddt/AI_you_tube_vids/blob/main/guidlines_graphby2.jpg?raw=true)\n",
    "<br>\n",
    "These guidelines come with help in how to best implement them, even so to translate them into practice, present quite a few challenges. \n",
    "A good starting point to understand how difficult this task can be, is Alexis Cook’s short introductory course to AI Fairness on Kaggle (https://www.kaggle.com/code/alexisbcook/ai-fairness). Alexis looks at ‘Fairness’, and immediately we are faced with the task of grounding it. Looking at one of the examples in the course, an AI model is built to select speakers for a conference, the attendees are expected to be 50% female-50% male. The AI can be trained to represent fairly the attendees mix by selecting 50% female 50% male speakers, but what if the experts in the fields are 70% female and 30% male, for the sake of argument, is it still fair to have a 50-50 split for the speakers? What if the experts in the field are 70% female but they have different scientific ranks, let’s say that 25% are ranked as leaders in the field whereas 20% of the males are ranked as leaders, is it fairer that the top people are selected as speakers irrespective of gender, or is it fair to reflect the gender split of the attendees, or the gender split of the overall experts group? But the problems starts even before you get to work, What if unbeknown to you, there was a problem with the attendees registration process and gender was either not recorded or recorded incorrectly, what if the system crashed and a lot of registrations were lost, what if the organizers had decided in advance that the attendee mix should be 1000 female 1000 male and once the limit is reached for a gender no more applications would be accepted from that gender, what if the organizers want to increase, let’s say the male audience so they set a limit for female attendees but not male? And what if for personal reason you do not like to identify with either sex and decide not to apply and therefore you do not exist in the dataset, which later might be used to directly train other AI systems or used to create synthetic datasets to train AI systems. And this is just a toy example, real life gets a lot more complicated a lot faster.<br> \n",
    "Regardless of the challenges, a fair, transparent, explainable and accountable AI is the best way we have to minimise risk and increase benefits to all of us, which is morally desirable, and, anyway, regulation is coming.<br>\n",
    "The EU is moving from guidelines to regulation, and developing an AI that cannot be sold or used in Europe because it does not comply with regulation may seriously hurt the bottom-line, already ChatGPT3 has failed to comply with privacy laws in Europe and has incurred a ban and a threat of future fines. And in the US regulation of artificial intelligence is on its way. Although legislation may vary from country to country, it should be clear, by now, to anyone in the AI field that the long arm of the Law is coming. <br>\n",
    "Regulation create as many fences as it does opportunities. Privacy is one of the areas guidelines focuses on;  in the EU there is a law commonly called ‘The right to be forgotten’(https://gdpr-info.eu/art-17-gdpr/), this law has implications downstream, as people choose to remove themselves from a dataset, the data may become skewed and introduce bias in AI models to be trained on it. However this offer also opportunities, there is already a move towards synthetic data, machine created data based on real data, this is cheaper, more energy efficient, and faster to create, and they can have the positive effect of adding another layer of protection to the real data point source, I.e you and me. Another possible benefit is that as demand pile up on synthetic data that meets the criteria imposed by regulation, may lead to datasets grounded in time and space, you know the place where we exist. \n",
    "The Demands for models to be Transparent, Explainable, and Accountable, will become law. Dan and Alexis in their course ‘Use Cases for Model Insights’ point out, there are hurdles, but the benefits could range from spotting and controlling bias early, improve the way data is collected. In addition there are promising studies in the way the weights used by a Neural-Networks could offer insights in how and why the Network produces outputs, and possibly help in uncovering persistent patterns. Demands for Transparency and Expandability may pave the way to systems that come with a full history, which, in the long run, would result in faster approval and deployment of AI technologies, as some of the component parts and how they interact would have already been proved to be compliant with the rules. Transparency and Expandability, may also help in devising fast and accurate technology to detect tempering or malicious outside interference on the systems.  <br>\n",
    "To conclude, what I covered here, is just a very small sample of the information available, the trends and the thinking that are going on. And although a huge amount of work has been done and is being done, there is still a lot more to do. Moving forward, we will uncover more risks and make more mistakes, but we will uncover, as well, more of the benefits that the technology has to offer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.304064,
   "end_time": "2023-07-04T08:11:20.667142",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-04T08:11:10.363078",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
